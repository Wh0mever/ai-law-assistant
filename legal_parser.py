#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏, –≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã —Å —Ç–æ—á–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
"""
import os
import re
import json
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class LegalArticle:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–∞"""
    article_number: str
    title: str
    content: str
    source_file: str
    chapter: Optional[str] = None
    section: Optional[str] = None
    part: Optional[str] = None
    paragraph_number: Optional[str] = None
    line_start: int = 0
    line_end: int = 0
    unique_id: str = ""
    
    def __post_init__(self):
        if not self.unique_id:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content_hash = hashlib.md5(
                f"{self.source_file}_{self.article_number}_{self.title}".encode()
            ).hexdigest()[:8]
            self.unique_id = f"{Path(self.source_file).stem}_{self.article_number}_{content_hash}"

@dataclass 
class LegalDocument:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    title: str
    source_file: str
    document_type: str
    articles: List[LegalArticle]
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

class LegalStructureParser:
    """–ü–∞—Ä—Å–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, txt_documents_dir: str = "txt_documents"):
        self.txt_dir = txt_documents_dir
        self.documents: List[LegalDocument] = []
        self.articles_index: Dict[str, LegalArticle] = {}
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        self.patterns = {
            # –°—Ç–∞—Ç—å–∏
            'article': [
                r'–°—Ç–∞—Ç—å—è\s+(\d+(?:\.\d+)*)\.\s*([^\n]+)',
                r'–°—Ç–∞—Ç—å—è\s+(\d+(?:\.\d+)*)\s*\.\s*([^\n]+)',
                r'–°—Ç–∞—Ç—å—è\s+(\d+(?:\.\d+)*)\s+([^\n]+)',
                r'—Å—Ç\.\s*(\d+(?:\.\d+)*)\s*\.?\s*([^\n]+)',
                r'(?:^|\n)(\d+(?:\.\d+)*)\.\s*([^\n]+?)(?=\n|$)'
            ],
            
            # –ì–ª–∞–≤—ã
            'chapter': [
                r'–ì–ª–∞–≤–∞\s+(\d+(?:\.\d+)*)\.\s*([^\n]+)',
                r'–ì–õ–ê–í–ê\s+(\d+(?:\.\d+)*)\.\s*([^\n]+)',
                r'–ì–ª–∞–≤–∞\s+([IVXLC]+)\.\s*([^\n]+)',
                r'–ì–õ–ê–í–ê\s+([IVXLC]+)\.\s*([^\n]+)'
            ],
            
            # –†–∞–∑–¥–µ–ª—ã
            'section': [
                r'–†–∞–∑–¥–µ–ª\s+([IVXLC]+)\.\s*([^\n]+)',
                r'–†–ê–ó–î–ï–õ\s+([IVXLC]+)\.\s*([^\n]+)',
                r'–†–∞–∑–¥–µ–ª\s+(\d+)\.\s*([^\n]+)',
                r'–†–ê–ó–î–ï–õ\s+(\d+)\.\s*([^\n]+)'
            ],
            
            # –ß–∞—Å—Ç–∏
            'part': [
                r'–ß–∞—Å—Ç—å\s+(\d+)\.\s*([^\n]+)',
                r'–ß–ê–°–¢–¨\s+(\d+)\.\s*([^\n]+)',
                r'—á–∞—Å—Ç—å\s+(\d+)\s*([^\n]*)'
            ],
            
            # –ü—É–Ω–∫—Ç—ã
            'paragraph': [
                r'^(\d+)\.\s*([^\n]+)',
                r'–ø—É–Ω–∫—Ç\s+(\d+)\s*\.?\s*([^\n]*)',
                r'–ø\.\s*(\d+)\s*\.?\s*([^\n]*)'
            ]
        }
    
    def detect_document_type(self, content: str, filename: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        content_lower = content.lower()
        filename_lower = filename.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É - –µ—Å–ª–∏ —Ñ–∞–π–ª –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –§–ó, —ç—Ç–æ –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω
        if '—Ñ–∑' in filename_lower or '/—Ñ–∑/' in filename_lower.replace('\\', '/'):
            return "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω"
        
        if any(word in content_lower for word in ['–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å', '–≥–∫ —Ä—Ñ']):
            return "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['—É–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å', '—É–∫ —Ä—Ñ']):
            return "–£–≥–æ–ª–æ–≤–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['—Ç—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å', '—Ç–∫ —Ä—Ñ']):
            return "–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π', '–∞–ø–∫ —Ä—Ñ']):
            return "–ê—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π', '–≥–ø–∫ —Ä—Ñ']):
            return "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['—É–≥–æ–ª–æ–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π', '—É–ø–∫ —Ä—Ñ']):
            return "–£–≥–æ–ª–æ–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π', '–∫–∞–ø —Ä—Ñ']):
            return "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–§"
        elif any(word in content_lower for word in ['–∫–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö', '–∫–æ–∞–ø —Ä—Ñ']):
            return "–ö–æ–¥–µ–∫—Å –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –†–§"
        elif any(word in content_lower for word in ['—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω', '—Ñ–∑']):
            return "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω"
        elif any(word in content_lower for word in ['–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ']):
            return "–°—É–¥–µ–±–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞"
        elif '–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–ø–ª—é—Å' in filename_lower:
            return "–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–ü–ª—é—Å –¥–æ–∫—É–º–µ–Ω—Ç"
        else:
            return "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    def extract_document_title(self, content: str, filename: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        lines = content.split('\n')[:20]  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
        for line in lines:
            line = line.strip()
            if len(line) > 10 and len(line) < 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                if any(word in line.upper() for word in [
                    '–ö–û–î–ï–ö–°', '–ó–ê–ö–û–ù', '–ü–û–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï', 
                    '–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï', '–†–û–°–°–ò–ô–°–ö–û–ô –§–ï–î–ï–†–ê–¶–ò–ò'
                ]):
                    return line
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        return Path(filename).stem.replace('_', ' ')
    
    def find_article_content(self, lines: List[str], start_idx: int, 
                           next_article_idx: Optional[int] = None) -> Tuple[str, int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏"""
        content_lines = []
        end_idx = next_article_idx if next_article_idx else len(lines)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –Ω–æ–º–µ—Ä–æ–º —Å—Ç–∞—Ç—å–∏
        for i in range(start_idx + 1, end_idx):
            line = lines[i].strip()
            
            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –µ–¥–∏–Ω–∏—Ü–µ
            if self.is_structural_element(line):
                end_idx = i
                break
                
            if line:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                content_lines.append(line)
        
        return '\n'.join(content_lines), end_idx
    
    def is_structural_element(self, line: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º"""
        for pattern_type, patterns in self.patterns.items():
            for pattern in patterns:
                if re.match(pattern, line.strip(), re.IGNORECASE):
                    return True
        return False
    
    def parse_document_structure(self, content: str, filename: str) -> LegalDocument:
        """–ü–∞—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        lines = content.split('\n')
        articles = []
        
        # –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        current_section = None
        current_chapter = None
        current_part = None
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        article_positions = []
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            for pattern in self.patterns['section']:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    current_section = f"–†–∞–∑–¥–µ–ª {match.group(1)}. {match.group(2)}"
                    break
            
            for pattern in self.patterns['chapter']:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    current_chapter = f"–ì–ª–∞–≤–∞ {match.group(1)}. {match.group(2)}"
                    break
            
            for pattern in self.patterns['part']:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    current_part = f"–ß–∞—Å—Ç—å {match.group(1)}. {match.group(2)}"
                    break
            
            # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
            for pattern in self.patterns['article']:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    article_number = match.group(1)
                    article_title = match.group(2).strip()
                    
                    article_positions.append({
                        'number': article_number,
                        'title': article_title,
                        'line_start': i,
                        'section': current_section,
                        'chapter': current_chapter,
                        'part': current_part
                    })
                    break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç–µ–π
        for i, article_info in enumerate(article_positions):
            next_article_line = (article_positions[i + 1]['line_start'] 
                               if i + 1 < len(article_positions) else None)
            
            content, end_line = self.find_article_content(
                lines, article_info['line_start'], next_article_line
            )
            
            if content.strip():  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                article = LegalArticle(
                    article_number=article_info['number'],
                    title=article_info['title'],
                    content=content.strip(),
                    source_file=filename,
                    section=article_info['section'],
                    chapter=article_info['chapter'],
                    part=article_info['part'],
                    line_start=article_info['line_start'],
                    line_end=end_line
                )
                articles.append(article)
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        doc_title = self.extract_document_title(content, filename)
        doc_type = self.detect_document_type(content, filename)
        
        document = LegalDocument(
            title=doc_title,
            source_file=filename,
            document_type=doc_type,
            articles=articles,
            metadata={
                'total_articles': len(articles),
                'parsed_at': 'unknown',
                'file_size': len(content)
            }
        )
        
        return document
    
    def parse_all_documents(self) -> Dict:
        """–ü–∞—Ä—Å–∏–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        txt_files = []
        
        if os.path.exists(self.txt_dir):
            txt_files = [f for f in os.listdir(self.txt_dir) 
                        if f.endswith('.txt') and not f.startswith('document_')]
        
        if not txt_files:
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ TXT —Ñ–∞–π–ª–æ–≤ –≤ {self.txt_dir}")
            return {}
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(txt_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        
        for txt_file in txt_files:
            file_path = os.path.join(self.txt_dir, txt_file)
            
            try:
                logger.info(f"–ü–∞—Ä—Å–∏–º: {txt_file}")
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–µ
                if content.startswith('# –î–æ–∫—É–º–µ–Ω—Ç:'):
                    lines = content.split('\n')
                    for i, line in enumerate(lines):
                        if not line.startswith('#') and line.strip():
                            content = '\n'.join(lines[i:])
                            break
                
                document = self.parse_document_structure(content, txt_file)
                self.documents.append(document)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—å–∏ –≤ –∏–Ω–¥–µ–∫—Å
                for article in document.articles:
                    self.articles_index[article.unique_id] = article
                
                logger.info(f"‚úÖ {txt_file}: –Ω–∞–π–¥–µ–Ω–æ {len(document.articles)} —Å—Ç–∞—Ç–µ–π")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {txt_file}: {e}")
        
        return self.generate_parsing_report()
    
    def generate_parsing_report(self) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –æ –ø–∞—Ä—Å–∏–Ω–≥–µ"""
        total_articles = sum(len(doc.articles) for doc in self.documents)
        
        doc_types = {}
        for doc in self.documents:
            doc_type = doc.document_type
            if doc_type not in doc_types:
                doc_types[doc_type] = []
            doc_types[doc_type].append(doc.title)
        
        report = {
            'total_documents': len(self.documents),
            'total_articles': total_articles,
            'document_types': doc_types,
            'articles_by_document': {
                doc.title: len(doc.articles) for doc in self.documents
            }
        }
        
        return report
    
    def save_parsed_data(self, output_file: str = "legal_structure.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        output_path = os.path.join(self.txt_dir, output_file)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        data = {
            'documents': [asdict(doc) for doc in self.documents],
            'articles_index': {k: asdict(v) for k, v in self.articles_index.items()},
            'metadata': {
                'total_documents': len(self.documents),
                'total_articles': len(self.articles_index),
                'parsing_date': 'unknown'
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_path}")
        return output_path
    
    def search_articles(self, query: str, max_results: int = 5) -> List[LegalArticle]:
        """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å—Ç–∞—Ç—å—è–º"""
        query_lower = query.lower()
        results = []
        
        for article in self.articles_index.values():
            score = 0
            
            # –ü–æ–∏—Å–∫ –≤ –Ω–æ–º–µ—Ä–µ —Å—Ç–∞—Ç—å–∏
            if query_lower in article.article_number.lower():
                score += 10
                
            # –ü–æ–∏—Å–∫ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            if query_lower in article.title.lower():
                score += 8
                
            # –ü–æ–∏—Å–∫ –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
            if query_lower in article.content.lower():
                score += 5
                
            # –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            if article.chapter and query_lower in article.chapter.lower():
                score += 3
                
            if score > 0:
                results.append((article, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        
        return [article for article, score in results[:max_results]]
    
    def get_article_reference(self, article: LegalArticle) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é"""
        ref_parts = []
        
        if article.section:
            ref_parts.append(article.section)
        if article.chapter:
            ref_parts.append(article.chapter)
        
        ref_parts.append(f"–°—Ç–∞—Ç—å—è {article.article_number}")
        
        if article.title:
            ref_parts.append(f'"{article.title}"')
        
        ref_text = " ‚Üí ".join(ref_parts)
        ref_text += f"\nüìé –ò—Å—Ç–æ—á–Ω–∏–∫: {article.source_file}#{article.unique_id}"
        
        return ref_text


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üèõÔ∏è –ü–∞—Ä—Å–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("=" * 50)
    
    parser = LegalStructureParser()
    
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    report = parser.parse_all_documents()
    
    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
    print(f"\nüìä –û–¢–ß–ï–¢ –û –ü–ê–†–°–ò–ù–ì–ï:")
    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {report['total_documents']}")
    print(f"‚öñÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {report['total_articles']}")
    
    print(f"\nüìã –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    for doc_type, titles in report['document_types'].items():
        print(f"  ‚Ä¢ {doc_type}: {len(titles)} —à—Ç.")
    
    print(f"\nüìà –°—Ç–∞—Ç–µ–π –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º:")
    for title, count in report['articles_by_document'].items():
        print(f"  ‚Ä¢ {title}: {count} —Å—Ç–∞—Ç–µ–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = parser.save_parsed_data()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞
    print(f"\nüîç –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞:")
    test_queries = ["–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–¥–æ–≥–æ–≤–æ—Ä", "—Å—É–¥", "–ø—Ä–∞–≤–æ"]
    
    for query in test_queries:
        results = parser.search_articles(query, max_results=2)
        if results:
            print(f"\nüîé '{query}':")
            for article in results:
                print(f"  üìÑ –°—Ç–∞—Ç—å—è {article.article_number}: {article.title}")
                print(f"     üìÅ {article.source_file}")
    
    print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")


if __name__ == "__main__":
    main() 